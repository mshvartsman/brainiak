{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A faster way of doing TFA\n",
    "\n",
    "The basic TFA generative model is: \n",
    "\n",
    "$$\n",
    "Y \\mid W, F \\sim \\mathcal{MN}(WF, \\sigma^2 I, I), \n",
    "$$\n",
    "\n",
    "where F is the matrix containing the factors. We can put a trivial i.i.d. normal prior on $W$, giving us the following marginal: \n",
    "\n",
    "$$\n",
    "Y \\mid F \\sim \\mathcal{MN}(0, \\sigma^2 I , F^T F + I). \n",
    "$$\n",
    "We can apply the matrix determinant and inversion lemmas to replace the $F^T F$ inverse with a much nicer $FF^T$ inverse, and compute the marginal log-likelihood pretty efficiently, autodiff through it, and optimize by L-BFGS-B. \n",
    "\n",
    "In contrast to vanilla TFA, this way of doing things: \n",
    "- Gets rid of the coordinate ascent bit completely: we find $F$ marginalizing over $W$ and then compute $W$ in closed form once. \n",
    "- Gets rid of finite differences, which is slow. \n",
    "- Gets rid of a massive jacobian that needs to be subsampled (there's a tradeoff here: we're not exploiting the least squares structure any more). \n",
    "- Can be computed on GPU if we'd like. \n",
    "\n",
    "It's also still compatible with naive voxelwise and TRwise subsampling as in regular TFA. \n",
    "\n",
    "And it's still theoretically compatible with HTFA (just not implemented yet). Let's see how it goes! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from brainiak.factoranalysis.tfa import TFA\n",
    "from brainiak.factoranalysis.fast_tfa import FastTFA, get_factormat\n",
    "\n",
    "def benchmark_tfa(v, t, k, noise, maxv, maxt, skip_slow=False, skip_subsampled=False):\n",
    "    R = np.random.randint(2, high=102, size=(v, 3)).astype(\"float64\")\n",
    "    centers = np.random.randint(2, high=102, size=(k, 3)).astype(\"float64\")\n",
    "    widths = np.abs(np.random.normal(loc=0, scale=np.std(R)**2, size=(k, 1)))\n",
    "    F = get_factormat(R, centers, widths).numpy()\n",
    "    W = np.random.normal(size=(t, k))\n",
    "    X = W @ F + np.random.normal(size=(t, v))*noise\n",
    "\n",
    "    if skip_slow is False:\n",
    "        print(\"\\nFull Fast TFA:\")\n",
    "        fast_tfa = FastTFA(k=k)\n",
    "        %timeit -n 1 -r 5 fast_tfa.fit(X, R)\n",
    "        fast_tfa_mse = np.mean((X - fast_tfa.W_ @ fast_tfa.F_)**2)\n",
    "        print(f\"MSE={fast_tfa_mse}\")\n",
    "\n",
    "        print(\"\\nFull TFA\")    \n",
    "        tfa_full = TFA(K=k, max_num_tr=t, max_num_voxel=v)\n",
    "        %timeit -n 1 -r 5 tfa_full.fit(X.T, R)\n",
    "        tfa_full_mse = np.mean((X.T - tfa_full.F_ @ tfa_full.W_)**2)\n",
    "        print(f\"MSE={tfa_full_mse}\")\n",
    "    \n",
    "    if skip_subsampled is False: \n",
    "        print(\"\\nSubsampled Fast TFA:\")\n",
    "        fast_tfa = FastTFA(k=k)\n",
    "        %timeit -n 1 -r 5 fast_tfa.fit(X, R, subsamp_size_v=maxv, subsamp_size_t=maxt, n_iter=10)\n",
    "        fast_tfa_mse = np.mean((X - fast_tfa.W_ @ fast_tfa.F_)**2)\n",
    "        print(f\"MSE={fast_tfa_mse}\")\n",
    "\n",
    "\n",
    "        print(\"\\nSubsampled regular TFA:\")\n",
    "        tfa_subsamp = TFA(K=k, max_num_tr=maxt, max_num_voxel=maxv)\n",
    "        %timeit -n 1 -r 5 tfa_subsamp.fit(X.T, R)\n",
    "        tfa_subsamp_mse = np.mean((X.T - tfa_subsamp.F_ @ tfa_subsamp.W_)**2)\n",
    "        print(f\"MSE={tfa_subsamp_mse}\")\n",
    "\n",
    "# I think this might force the JIT to do its thing so we can have clean timings below    \n",
    "R = np.random.randint(2, high=102, size=(10, 3)).astype(\"float64\")\n",
    "centers = np.random.randint(2, high=102, size=(3, 3)).astype(\"float64\")\n",
    "widths = np.abs(np.random.normal(loc=0, scale=np.std(R)**2, size=(3, 1)))\n",
    "F = get_factormat(R, centers, widths).numpy()\n",
    "W = np.random.normal(size=(5, 3))\n",
    "X = W @ F + np.random.normal(size=(5, 10))\n",
    "ftfa = FastTFA(k=3)\n",
    "_ = ftfa.fit(X, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Fast TFA:\n",
      "235 ms ± 103 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=0.016805094136183363\n",
      "\n",
      "Full TFA\n",
      "21.3 s ± 146 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=0.017925624628788255\n"
     ]
    }
   ],
   "source": [
    "# tiny problem with basically no noise, sanity check, subsampling doesn't make sense\n",
    "benchmark_tfa(v=200, t=100, k=5, noise=0.1, maxv=200, maxt=100, skip_subsampled=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Fast TFA:\n",
      "The slowest run took 6.14 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "168 ms ± 119 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=0.9855854473253423\n",
      "\n",
      "Full TFA\n",
      "7.82 s ± 99.1 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=0.9857954102123114\n"
     ]
    }
   ],
   "source": [
    "# same tiny problem with a bit more noise\n",
    "benchmark_tfa(v=200, t=100, k=5, noise=1, maxv=200, maxt=100, skip_subsampled=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Fast TFA:\n",
      "2.71 s ± 295 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=0.9719086009432499\n",
      "\n",
      "Full TFA\n",
      "1min 35s ± 1.48 s per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=1.011639365409478\n",
      "\n",
      "Subsampled Fast TFA:\n",
      "4.66 s ± 463 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=0.9825072316601544\n",
      "\n",
      "Subsampled regular TFA:\n",
      "19.9 s ± 98.9 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=1.016532650335811\n"
     ]
    }
   ],
   "source": [
    "# modest subsampling, no benefit for fast TFA but big benefit for regular TFA\n",
    "benchmark_tfa(v=500, t=200, k=15, noise=1, maxv=200, maxt=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subsampled Fast TFA:\n",
      "4.91 s ± 845 ms per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=0.995978002703193\n",
      "\n",
      "Subsampled regular TFA:\n",
      "38.8 s ± 6.82 s per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=1.0244633334503621\n"
     ]
    }
   ],
   "source": [
    "# no patience to try the non-subsampled one\n",
    "benchmark_tfa(v=1000, t=200, k=15, noise=1, skip_slow=True, maxv=250, maxt=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subsampled Fast TFA:\n",
      "5.62 s ± 1.63 s per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=1.0009771281581075\n",
      "\n",
      "Subsampled regular TFA:\n",
      "53.8 s ± 2.38 s per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=1.0193063341196682\n"
     ]
    }
   ],
   "source": [
    "# ROI size\n",
    "benchmark_tfa(v=2000, t=500, k=20, noise=1, skip_slow=True, maxv=250, maxt=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Subsampled Fast TFA:\n",
      "21 s ± 1.29 s per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=1.0219173418617433\n",
      "\n",
      "Subsampled regular TFA:\n",
      "2min 54s ± 12 s per loop (mean ± std. dev. of 5 runs, 1 loop each)\n",
      "MSE=1.029504454343515\n"
     ]
    }
   ],
   "source": [
    "# full brain size\n",
    "benchmark_tfa(v=100000, t=1250, k=50, noise=1, skip_slow=True, maxv=250, maxt=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "Fast TFA is consistently more accurate at reconstruction (by integrating over W), is faster than regular without subsampling TFA but not nearly as fast as the subsampling version of regular-TFA for more practical problems. \n",
    "\n",
    "With subsampling, fast TFA is still more accurate than regular TFA (subsampling or otherwise) and substantially faster than either version. Asymptotically it seems ~10x faster than subsampling TFA with the same subsampling size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:brainiak] *",
   "language": "python",
   "name": "conda-env-brainiak-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
